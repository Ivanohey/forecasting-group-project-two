# Data cleaning

```{r data_merge}
#Flights data
data_2020 <- read.csv("data/2020-States.csv")
data_2021 <- read.csv("data/2021-States.csv")
data_2022 <- read.csv("data/2022-States.csv")
data_2023 <- read.csv("data/2023-States.csv")

#Merge all flights datasets together
data_2019 <- select(data_2020, Entity, Day.2019, Flights.2019..Reference.) %>% rename(Day = Day.2019, Flights = Flights.2019..Reference.)
data_2020 <- select(data_2020, Entity, Day, Flights)
data_2021 <- select(data_2021, Entity, Day, Flights)
data_2022 <- select(data_2022, Entity, Day, Flights)
data_2023 <- select(data_2023, Entity, Day, Flights)
data_all_years <- rbind(data_2019, data_2020, data_2021, data_2022, data_2023)

#Select only data where country == switzerland
flights <- data_all_years[data_all_years$Entity == 'Switzerland',]
flights$year_month = format(as.Date(flights$Day), "%Y-%m")

flights <- flights %>% select(Day,Flights) %>% set_names(c("Date", "Flights"))

#Convert the "Date" column to a date format:
#flights$Date <- parse_date_time(flights$Date, orders = c("%d-%b-%y", "%b %d, %Y"))
flights <- flights %>% mutate(Date = as.Date(Date, format = "%Y-%m-%d"))

#This shows that date is correctly converted to date type
str(flights)

#there are a bunch of duplicated rows, i guess because of the way the data was imported? 
duplicates(flights)

#This will remove all the duplicated rows, and only keep one of them
flights <- flights %>% 
  distinct()

#now there's no more duplicates
duplicates(flights)


```

```{r}
#Weather data
weather <- read_excel("data/switzerland weather.xlsx")

#Select appropriate columns (which covariates we will use)

weather <- weather %>% select(datetime,temp, precip, snow, windspeed, visibility, humidity)

#change column names to match with other datasets
colnames(weather)[1] <- "Date"

#Convert the "Date" column to a date format: (Previous type was POSIXct first time i'm seeing that)
#weather$Date <- parse_date_time(weather$Date, orders = c("%d-%b-%y", "%b %d, %Y"))
weather <- weather %>% mutate(Date = as.Date(Date))

#Some summary statistics for all columns
#weather %>% dfSummary(style = "grid")

str(weather)

#There are no duplicates here
duplicates(weather)

#any(is.na(weather))

#visibility data for 2022-2023 looks very sus
ggplot(weather, aes(x = Date, y = visibility)) +
  geom_line() +
  labs(title = "Visibility over time", x = "Date", y = "Visibility")

```

```{r}
#Oil data 

oil <- read.csv("data/BrentOilPrices.csv")

#Check for NA's, there's none
#sum(is.na(oil))

#Convert the "Date" column to a date format:
oil$Date <- parse_date_time(oil$Date, orders = c("%d-%b-%y", "%b %d, %Y"))

#Convert data to date type
oil <- oil %>% mutate(Date = as.Date(Date))

#check it's of the right type 
str(oil)

#Summary statistics 
oil %>% dfSummary(style = "grid")

#Plot the time series 
ggplot(oil, aes(x = Date, y = Price)) +
  geom_line() +
  labs(title = "Brent Oil Prices (all data)", x = "Date", y = "Price")

#convert to tsibble object -> NOT YET
#oil <- oil %>%
  #mutate(Date = yearmonth(Date)) %>% #need to do this before converting to get proper format
  #as_tsibble(index = Date)

#Select appropriate dates
oil <- oil %>%
  mutate(Date = as.Date(Date)) %>%
  filter(Date > as.Date("2019-01-01"))

#Plot the time series 
ggplot(oil, aes(x = Date, y = Price)) +
  geom_line() +
  labs(title = "Brent Oil Prices 2019 onwards", x = "Date", y = "Price")




#I do the following, becauase I realized that even though there are not explicit NA's, there are some date rows missing, and I need to create them... 


# create a complete sequence of dates from the minimum to maximum date
all_dates <- seq(min(oil$Date), max(oil$Date), by = "day")

# use complete() to create rows for the missing dates
oil <- oil %>% 
  complete(Date = all_dates)

# replace missing prices with NA
oil$Price[is.na(oil$Price)] <- NA

#There are 426 missing values now that I have completed the oil dataset... 
sum(is.na(oil))

#there are no duplicates here
duplicates(oil)

```

```{r}
#Trying with another oil dataset... THIS WAS EVEN WORSE

#newoil <- read.csv('data/crudeoil.csv')

#Converting date column to date type + format
#newoil <- newoil %>%
#  mutate(Date = as.Date(Date, format = "%m/%d/%Y"))

#Select relevant columns
#newoil <- newoil %>% select(Date,Close.Last)

#Select relevant dates
#newoil <- newoil %>%
#  mutate(Date = as.Date(Date)) %>%
#  filter(Date > as.Date("2019-01-01"))

#Plot the time series 
#ggplot(newoil, aes(x = Date, y = Close.Last)) +
#  geom_line() +
#  labs(title = "Oil Prices 2019 onwards", x = "Date", y = "Price")

#Check for NA's -> There are no NA's for the moment
#sum(is.na(newoil))


```


## Merging the datasets

```{r}
#Merging datasets together 
data <- flights %>%
  left_join(oil, by = "Date") %>%
  left_join(weather, by = "Date")

data <- data %>% 
  filter(Date < as.Date("2022-11-14"))

#Convert to tsibble object  
ts_data <- data %>%
  as_tsibble(index = Date)


#Plot the time series 
ts_data %>% autoplot(Price) +
  labs(title = "Brent Oil Prices 2019 onwards", x = "Date", y = "Price")

#Info on our variables
str(ts_data)

```

As we can see there's a lot of missing values for the price variable. I will try to impute them using the KNN method: 

I discovered the imputeTS package, which includes various imputation methods, as well as some statistics for the missing values. Here are some of them: 

```{r}
library(imputeTS)

#This graph looks terrible but it shows you all the missing data 
ggplot_na_distribution(ts_data$Price)

#missing data statistics -> actually quite useful to see what's going on
statsNA(ts_data$Price)

#I'm choosingthe moving average method to impute the data.
ts_data$Price <- na_ma(ts_data$Price)

#Plot the time series 
ts_data %>% autoplot(Price) +
  labs(title = "Brent Oil Prices 2019 onwards - IMPUTED", x = "Date", y = "Price")

```
In the imputeTS package, there are a lot of methods to impute the data, and after trying some we decided to stick with the weighted moving average. This is because most frequently (185 times) the gap is only 2 NA values long. There are 13 cases where the gap is 3 NA's long, and only 4 cases where the gap is 4 NA's long. This means that using the weighted moving average is providing a very good approximation of the price data. 

If the gaps were longer and the NA's were all concentrated in one big area, then we would consider using other imputation methods, but this is really not the case. The missing values seem to be spread all over the data, so the moving weighted averate seems to be doing a great job. 

Finally, let's check to see if there's any missing data after imputation

```{r}
#Final check
any(is.na(ts_data)) #seems there are still some NA's

# Identify rows with missing values
na_rows <- !complete.cases(ts_data)

# Print rows with missing values
ts_data[na_rows,]

```
It looks like there are 6 rows with missing values in the visibility column. I will just use the same same function as previously to impute them. 

```{r}
#I'm choosing the moving average method to impute the data.
ts_data$visibility <- na_ma(ts_data$visibility)

any(is.na(ts_data))
```

